---
title: 'Chapter 5: Dimensionality reduction techniques'
author: "Mikko Salonen"
date: "21 February 2017"
output: html_document
---

# Chapter 5: Dimensionality reduction techniques

### Preliminaries

Let's first load the data we produced with the create_human.R script. This data is a collection of gross national income, expected education, life expectancy, education and labor shares between genders, maternal mortality rate, adolescent birth rate, and female political participation rate. In other words this data describes human development in countries.

We can see from the first plot below that GNI, maternal mortality rate, and adolescent birth rate are skewed and right tailed. High kurtosis and left tailed variables are: education ratio, labor ratio, and life expectancy. Expected education is distributed fairly similarly to a normal distribution.

From the second plot we can see that maternal mortality rate and adolescent birth rate are correlated and negatively correlated to high development variables such as: education ratio, GNI, and life expectancy.

```{r, echo=FALSE, warning= FALSE, tidy= TRUE}
human <- read.csv(file = "data/human.csv",row.names = "X")
head(human)
summary(human)
dim(human)
str(human)

library(GGally)
library(corrplot)
library(tidyr)
library(dplyr)

# visualize the 'human_' variables
ggpairs(human)
```

```{r, warning= FALSE, tidy= TRUE}

# compute the correlation matrix and visualize it with corrplot
cor(human) %>% round(digits=2) %>% corrplot.mixed(tl.cex = 0.7,number.cex=0.7,order="hclust",addrect=2)
```


<!-- Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points) -->

### Principal component analysis (PCA)

<!-- Perform principal component analysis (PCA) on the not standardized human data. Show the variability captured by the principal components. Draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables. (0-2 points) -->
Let's perform a PCA on the data. As we can see, without standardization the plot is quite a mess.

```{r, echo=FALSE, warning= FALSE, tidy= TRUE}
# perform principal component analysis (with the SVD method)
pca_human_nstd <- prcomp(human)

# rounded percetanges of variance captured by each PC
pca_pr <- round(1*summary(pca_human_nstd)$importance[2, ], digits = 5)*100

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

```

```{r, warning= FALSE, tidy= TRUE}
# draw a biplot of the principal component representation and the original variables
biplot(pca_human_nstd, choices = 1:2, cex = c(0.4, 0.4), col = c("grey60","navyblue"), xlab = pc_lab[1], ylab = pc_lab[2])

```

Let's standardize the data now and interprit the results. Without standardisation GNI seems to be orthogonal to all other components, this is because it's variance is the largest. With standardisation we can see more clearly which components are correlated and which atribute to principal components 1 and 2. Again, the correlations we interpreted before hold. Since female participation and labor ratio point up, this means they are orhogonal, or do not correlate with other variables in the data. 

This results seems appropriate, if in a country the child mortality rate is high or the mothers die younger, it is plausible that this has an effect to life expectancy directly but indirectly to expected education rate and therefore to gross national income. One could go as far as to say that the contribution of mothers or more generally females in an important factor in development (PC1). Other part contributing to welfare are the highly correlated components such as GNI, life expectancy, and education. Furhtermore education of females.

<!-- Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions in you plots where you describe the results by using not just your variable names, but the actual phenomenons they relate to. (0-4 points) -->

```{r, echo=FALSE, warning= FALSE, tidy= TRUE}
# standardize the variables
human_std <- scale(human)

# print out summaries of the standardized variables
summary(human_std)

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)

# rounded percetanges of variance captured by each PC
pca_pr <- round(1*summary(pca_human)$importance[2, ], digits = 5)*100

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

```

```{r, warning= FALSE, tidy= TRUE}

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.4, 0.4), col = c("grey60","navyblue"), xlab = pc_lab[1], ylab = pc_lab[2])

```

<!-- Give your personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data. (0-2 points) -->

### Multiple Correspondence Analysis

Next we do a MCA on the tea dataset. There are 300 observations and 36 variables. Let's cut down on the variables we analyze to a more manageable rate. We include the type of tea (black, green etc.), with or without bonuses (lemon, milk etc.), type of tea package (bag, loose etc.), price (cheap, upscale etc.), with or without sugar,where tea is enjoyed (store, tea shop etc.), and whether or not drinking is done with breakfast.

```{r,echo=FALSE, warning= FALSE, tidy= TRUE}
library(FactoMineR)
library(ggplot2)
data(tea)
dim(tea)
str(tea)
#summary(tea)

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how","price", "sugar", "where", "breakfast")

# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
#summary(tea_time)
str(tea_time)
#tea_time
#how, price, where
library(plyr)
#levels(tea_time$how)
tea_time$how <- mapvalues(tea_time$how, from = c("tea bag", "tea bag+unpackaged", "unpackaged"), to = c("bag", "both", "loose")) 
#levels(tea_time$how)

#levels(tea_time$price)
tea_time$price <- mapvalues(tea_time$price, from = c("p_branded","p_cheap","p_private label","p_unknown","p_upscale","p_variable"),to = c("branded", "cheap", "private","unknown","upscale","variable")) 
#levels(tea_time$price)

#levels(tea_time$where)
tea_time$where <- mapvalues(tea_time$where, from = c("chain store","chain store+tea shop","tea shop" ),to = c("store", "both", "shop"))
#levels(tea_time$where)
```
```{r, warning= FALSE, tidy= TRUE}
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 25, hjust = 1, size = 8))
```

```{r, echo=FALSE, warning= FALSE, tidy= TRUE}
# tea_time is available

# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
```

```{r, warning= FALSE, tidy= TRUE}
# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```


First we plot the frequencies of the variables. As we can see, most people use Earl Grey bag tea, drink it alone in a store (home excluded), from all kinds of price ranges, every now and then adding sugar. From the second plot we can see the illustration of the MCA model. We can see that exclusive group ( drinking green upscale loose tea in a tea shop) is more close to each other. The second group noting away from the main group is casual drinkers whom drink in a shop or store a variable price range bag or as loose tea added with a secret ingredient (other, possibly alcohol, I'm thinking about rum toddy here!). I excluded the other MCA graphs as they added little or no further information to the analysis.

<!-- Load the tea dataset from the package Factominer. Explore the data briefly: look at the structure and the dimensions of the data and visualize it. Then do Multiple Correspondence Analysis on the tea data (or to a certain columns of the data, itâ€™s up to you). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots. (0-4 points) -->
